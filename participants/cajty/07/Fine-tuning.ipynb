{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "38769582455b488fada1f780855b7292": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65a054f88bb24d4e8f34a84021819ca0",
       "IPY_MODEL_c599694d2fdf41e7936003522c485e52",
       "IPY_MODEL_89b877c026cf43e78889b271f75960fc"
      ],
      "layout": "IPY_MODEL_484e1cc126b0452d8d524c3228dea10a"
     }
    },
    "65a054f88bb24d4e8f34a84021819ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3efe4f4cf2247e480b3b955094b531a",
      "placeholder": "​",
      "style": "IPY_MODEL_57e374e6d7c14fbaa7b6f7e3bec41b48",
      "value": "Map: 100%"
     }
    },
    "c599694d2fdf41e7936003522c485e52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_851f6c816ae6474381038349873bb562",
      "max": 2378,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e722a70527c74cca82ead600c14e0c2e",
      "value": 2378
     }
    },
    "89b877c026cf43e78889b271f75960fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_194682d360cf4dc2bcfb783812f8d3fe",
      "placeholder": "​",
      "style": "IPY_MODEL_9da788e36f974c18bb6727863c1d5b46",
      "value": " 2378/2378 [00:01&lt;00:00, 1673.68 examples/s]"
     }
    },
    "484e1cc126b0452d8d524c3228dea10a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3efe4f4cf2247e480b3b955094b531a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57e374e6d7c14fbaa7b6f7e3bec41b48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "851f6c816ae6474381038349873bb562": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e722a70527c74cca82ead600c14e0c2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "194682d360cf4dc2bcfb783812f8d3fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da788e36f974c18bb6727863c1d5b46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6c03ec3e6554e119333da17939b1c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a13f81f0664d42648d1b4f8b1060cf09",
       "IPY_MODEL_bcbadeafc4d74f3e94c6bc5dca944517",
       "IPY_MODEL_6b56291335c943a3aaa4a393e0f8df1f"
      ],
      "layout": "IPY_MODEL_4bc6c404f1c94aa2b6791a2c51f47381"
     }
    },
    "a13f81f0664d42648d1b4f8b1060cf09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e26ac518c1a74f66a768d66b75e563ef",
      "placeholder": "​",
      "style": "IPY_MODEL_93f2949066364b57a771bac7421226ac",
      "value": "Truncating train dataset: 100%"
     }
    },
    "bcbadeafc4d74f3e94c6bc5dca944517": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97bf57ce589a42d280b18aedb69845b3",
      "max": 2378,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8aa97e02aba4474cb29a46f70ca25fda",
      "value": 2378
     }
    },
    "6b56291335c943a3aaa4a393e0f8df1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d05819cd9a414f6a849990f4d1f5ddfa",
      "placeholder": "​",
      "style": "IPY_MODEL_29680f36c8464023a9e2fa3fec7aa784",
      "value": " 2378/2378 [00:00&lt;00:00, 80133.49 examples/s]"
     }
    },
    "4bc6c404f1c94aa2b6791a2c51f47381": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e26ac518c1a74f66a768d66b75e563ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93f2949066364b57a771bac7421226ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97bf57ce589a42d280b18aedb69845b3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aa97e02aba4474cb29a46f70ca25fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d05819cd9a414f6a849990f4d1f5ddfa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29680f36c8464023a9e2fa3fec7aa784": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Moroccan Darija SmolLM2 Fine-tuning\n",
    "\n",
    "A comprehensive fine-tuning implementation that adapts **SmolLM2-135M-Instruct** to understand and generate Moroccan Darija (Arabic dialect) using high-quality Q&A datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project fine-tunes the `HuggingFaceTB/SmolLM2-135M-Instruct` model on over **2,300 Moroccan Darija question-answer pairs** from the `Lyte/Moroccan-Darija-QA` dataset. The model learns to respond to questions in Darija across multiple domains, including business, culture, health, food, technology, and daily conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kKAUzEilylH",
    "outputId": "8f89142f-b82d-42c4-9f0a-7083eacc94c0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mon Sep 15 13:32:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Moroccan Darija Fine-tuning for SmolLM2-135M using Lyte/Moroccan-Darija-QA\n",
    "# Optimized for Google Colab with high-quality Q&A dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# MOROCCAN DARIJA Q&A DATASET INTEGRATION\n",
    "# ============================================================================\n",
    "\n",
    "def load_moroccan_darija_qa_dataset():\n",
    "    \"\"\"Load the high-quality Moroccan Darija Q&A dataset from Lyte\"\"\"\n",
    "\n",
    "    print(\"🇲🇦 Loading Moroccan Darija Q&A Dataset...\")\n",
    "\n",
    "    try:\n",
    "        # Load different configurations\n",
    "        dataset_default = load_dataset(\"Lyte/Moroccan-Darija-QA\", name=\"default\")\n",
    "        dataset_translated = load_dataset(\"Lyte/Moroccan-Darija-QA\", name=\"translated\")\n",
    "        dataset_reasoning = load_dataset(\"Lyte/Moroccan-Darija-QA\", name=\"reasoning\")\n",
    "\n",
    "        print(f\"✅ Default config loaded: {len(dataset_default['train'])} examples\")\n",
    "        print(f\"✅ Translated config loaded: {len(dataset_translated['train'])} examples\")\n",
    "        print(f\"✅ Reasoning config loaded: {len(dataset_reasoning['train'])} examples\")\n",
    "\n",
    "        # Preview the data structure\n",
    "        print(\"\\n📊 Dataset Preview:\")\n",
    "        for i, example in enumerate(dataset_default['train']):\n",
    "            if i < 3:\n",
    "                print(f\"Question: {example['question']}\")\n",
    "                print(f\"Answer: {example['answer'][:100]}...\")\n",
    "                print(f\"Category: {example['category']}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "        return {\n",
    "            'default': dataset_default,\n",
    "            'translated': dataset_translated,\n",
    "            'reasoning': dataset_reasoning\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Q&A dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_darija_qa_for_training(qa_datasets):\n",
    "    \"\"\"Convert Q&A datasets to instruction-following format for training\"\"\"\n",
    "\n",
    "    if qa_datasets is None:\n",
    "        return None\n",
    "\n",
    "    training_examples = []\n",
    "\n",
    "    # Process default configuration (main dataset)\n",
    "    print(\"\\n🔄 Processing default Q&A dataset...\")\n",
    "    default_data = qa_datasets['default']['train']\n",
    "\n",
    "    for example in default_data:\n",
    "        question = example['question'].strip()\n",
    "        answer = example['answer'].strip()\n",
    "        category = example['category']\n",
    "\n",
    "        # Skip very short answers\n",
    "        if len(answer) < 20:\n",
    "            continue\n",
    "\n",
    "        # Create instruction format\n",
    "        training_examples.append({\n",
    "            'prompt': question,\n",
    "            'response': answer,\n",
    "            'category': category\n",
    "        })\n",
    "\n",
    "    # Process reasoning configuration (adds thinking process)\n",
    "    print(\"🧠 Processing reasoning Q&A dataset...\")\n",
    "    reasoning_data = qa_datasets['reasoning']['train']\n",
    "\n",
    "    for example in reasoning_data:\n",
    "        question = example['question'].strip()\n",
    "        answer = example['answer'].strip()\n",
    "        category = example['category']\n",
    "\n",
    "        # Skip very short answers\n",
    "        if len(answer) < 20:\n",
    "            continue\n",
    "\n",
    "        training_examples.append({\n",
    "            'prompt': question,\n",
    "            'response': answer,\n",
    "            'category': f\"{category}_reasoning\"\n",
    "        })\n",
    "\n",
    "    # Optionally include some translated examples for diversity\n",
    "    print(\"🌍 Adding translated examples for variety...\")\n",
    "    translated_data = qa_datasets['translated']['train']\n",
    "\n",
    "    # Take a sample of translated data (not all to maintain Darija focus)\n",
    "    sample_size = min(200, len(translated_data))\n",
    "    translated_sample = random.sample(list(translated_data), sample_size)\n",
    "\n",
    "    for example in translated_sample:\n",
    "        question = example['question'].strip()\n",
    "        answer = example['answer'].strip()\n",
    "        category = example['category']\n",
    "\n",
    "        training_examples.append({\n",
    "            'prompt': question,\n",
    "            'response': answer,\n",
    "            'category': f\"{category}_translated\"\n",
    "        })\n",
    "\n",
    "    print(f\"✅ Total training examples prepared: {len(training_examples)}\")\n",
    "\n",
    "    # Show category distribution\n",
    "    categories = {}\n",
    "    for ex in training_examples:\n",
    "        cat = ex['category']\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "    print(\"\\n📈 Category Distribution:\")\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        print(f\"  {cat}: {count} examples\")\n",
    "\n",
    "    return Dataset.from_list(training_examples)\n",
    "\n",
    "def add_conversational_examples():\n",
    "    \"\"\"Add some basic conversational examples to complement the Q&A data\"\"\"\n",
    "\n",
    "    darija_conversations = [\n",
    "        {\n",
    "            \"prompt\": \"كيف داير؟\",\n",
    "            \"response\": \"الحمد لله، بخير. أنت كيف داير؟\",\n",
    "            \"category\": \"greeting\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"شنو كتعمل اليوم؟\",\n",
    "            \"response\": \"كنخدم فالبيت وكنقرا شوية. أنت شنو كتعمل؟\",\n",
    "            \"category\": \"daily_conversation\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"واش بغيتي تشرب أتاي؟\",\n",
    "            \"response\": \"إيه بغيت، شكرا ليك. أتاي بالنعناع زين بزاف.\",\n",
    "            \"category\": \"hospitality\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"شكرا ليك بزاف\",\n",
    "            \"response\": \"العفو، ماشي مشكل. كلشي ديال الخير.\",\n",
    "            \"category\": \"politeness\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"بصحتك\",\n",
    "            \"response\": \"الله يعطيك الصحة. شكرا ليك.\",\n",
    "            \"category\": \"well_wishes\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"أشنو كتحب فالماكلة المغريبية؟\",\n",
    "            \"response\": \"كنحب الطاجين والكسكس والپاستيطا. والحلويات مغريبية زينين بزاف.\",\n",
    "            \"category\": \"food_conversation\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"واش كتعرف تطيب؟\",\n",
    "            \"response\": \"إيه كنعرف نطيب شوية. كنطيب الطاجين والحريرة مزيان.\",\n",
    "            \"category\": \"cooking\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"السلام عليكم\",\n",
    "            \"response\": \"وعليكم السلام ورحمة الله وبركاته. أهلا وسهلا.\",\n",
    "            \"category\": \"religious_greeting\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"➕ Added {len(darija_conversations)} conversational examples\")\n",
    "    return Dataset.from_list(darija_conversations)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED DATASET PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_complete_darija_dataset():\n",
    "    \"\"\"Combine Q&A dataset with conversational examples\"\"\"\n",
    "\n",
    "    # Load the main Q&A dataset\n",
    "    qa_datasets = load_moroccan_darija_qa_dataset()\n",
    "    if qa_datasets is None:\n",
    "        print(\"❌ Failed to load Q&A dataset\")\n",
    "        return None\n",
    "\n",
    "    # Prepare Q&A data for training\n",
    "    qa_training_data = prepare_darija_qa_for_training(qa_datasets)\n",
    "\n",
    "    # Add conversational examples\n",
    "    conversation_data = add_conversational_examples()\n",
    "\n",
    "    # Combine datasets\n",
    "    if qa_training_data and conversation_data:\n",
    "        combined_dataset = concatenate_datasets([qa_training_data, conversation_data])\n",
    "    else:\n",
    "        combined_dataset = qa_training_data or conversation_data\n",
    "\n",
    "    print(f\"\\n🎯 Final dataset size: {len(combined_dataset)} examples\")\n",
    "    return combined_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETUP FOR DARIJA Q&A\n",
    "# ============================================================================\n",
    "\n",
    "def setup_darija_qa_training():\n",
    "    \"\"\"Setup training configuration optimized for Darija Q&A and Colab\"\"\"\n",
    "\n",
    "    # Check GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"🚀 Available GPU memory: {gpu_memory:.1f}GB\")\n",
    "\n",
    "        # Adjust batch size based on memory\n",
    "        if gpu_memory < 16:  # T4 in Colab\n",
    "            batch_size = 1\n",
    "            grad_accumulation = 8\n",
    "        else:\n",
    "            batch_size = 2\n",
    "            grad_accumulation = 4\n",
    "    else:\n",
    "        print(\"⚠️  Running on CPU - training will be slower\")\n",
    "        batch_size = 1\n",
    "        grad_accumulation = 8\n",
    "\n",
    "    # Adjusted for larger, higher-quality dataset\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=grad_accumulation,\n",
    "        warmup_steps=100,  # More warmup for larger dataset\n",
    "        max_steps=500,     # More steps for comprehensive Q&A learning\n",
    "        learning_rate=1e-5,  # Lower LR for stability with quality data\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=42,\n",
    "        output_dir=\"darija_qa_model_outputs\",\n",
    "        save_steps=150,\n",
    "        dataloader_drop_last=True,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    return training_args\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED TOKENIZATION FOR Q&A FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_darija_qa_tokenization(dataset, tokenizer, max_length):\n",
    "    \"\"\"Prepare tokenization specifically for Darija Q&A format\"\"\"\n",
    "\n",
    "    def tokenize_qa_examples(examples):\n",
    "        \"\"\"Tokenize Q&A examples with proper chat format\"\"\"\n",
    "\n",
    "        texts = []\n",
    "        for prompt, response in zip(examples[\"prompt\"], examples[\"response\"]):\n",
    "            # Create chat format for Q&A\n",
    "            conversation = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": response}\n",
    "            ]\n",
    "\n",
    "            # Apply chat template\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "\n",
    "        # Tokenize with appropriate settings for Q&A\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_qa_examples,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_darija_qa_smollm():\n",
    "    \"\"\"Main function to train SmolLM2 on high-quality Darija Q&A data\"\"\"\n",
    "\n",
    "    print(\"🇲🇦 Starting Moroccan Darija Q&A Fine-tuning for SmolLM2-135M\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # 1. Load model and tokenizer\n",
    "    print(\"\\n1️⃣ Loading base model...\")\n",
    "    model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(f\"✅ Model loaded: {model_name}\")\n",
    "    print(f\"✅ Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "    # 2. Prepare dataset\n",
    "    print(\"\\n2️⃣ Preparing Darija Q&A datasets...\")\n",
    "    darija_dataset = prepare_complete_darija_dataset()\n",
    "\n",
    "    if darija_dataset is None:\n",
    "        print(\"❌ Failed to prepare dataset\")\n",
    "        return None\n",
    "\n",
    "    # 3. Tokenize dataset\n",
    "    print(\"\\n3️⃣ Tokenizing data...\")\n",
    "    max_seq_length = 512  # Increased for Q&A format\n",
    "    tokenized_dataset = prepare_darija_qa_tokenization(darija_dataset, tokenizer, max_seq_length)\n",
    "\n",
    "    print(f\"✅ Tokenization complete. Dataset size: {len(tokenized_dataset)}\")\n",
    "\n",
    "    # 4. Setup training\n",
    "    print(\"\\n4️⃣ Setting up training...\")\n",
    "    training_args = setup_darija_qa_training()\n",
    "\n",
    "    # 5. Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # 6. Start training\n",
    "    print(\"\\n5️⃣ Starting training...\")\n",
    "    print(\"🕐 Training will take approximately 25-35 minutes on Colab T4\")\n",
    "    print(\"📚 Learning from 2000+ Darija Q&A pairs...\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"✅ Training completed!\")\n",
    "    return trainer\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED TESTING FOR Q&A MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def test_darija_qa_model(trainer):\n",
    "    \"\"\"Test the fine-tuned model on various Darija Q&A scenarios\"\"\"\n",
    "\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Create pipeline\n",
    "    darija_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.processing_class,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "    # Test prompts covering different categories from the dataset\n",
    "    test_prompts = [\n",
    "        # Business\n",
    "        \"واش التجارة مربحة فالمغرب؟\",\n",
    "        \"كيفاش نفتح مقاولة صغيرة؟\",\n",
    "\n",
    "        # Health\n",
    "        \"شنو هوما فوائد الرياضة؟\",\n",
    "        \"كيفاش نحافظ على صحتي؟\",\n",
    "\n",
    "        # Food\n",
    "        \"كيفاش نطيب الطاجين؟\",\n",
    "        \"شنو أحسن ماكلة مغريبية؟\",\n",
    "\n",
    "        # Culture\n",
    "        \"شنو هوما التقاليد المغريبية؟\",\n",
    "        \"علاش عيد الفطر مهم؟\",\n",
    "\n",
    "        # Daily conversation\n",
    "        \"كيف داير؟\",\n",
    "        \"شنو كتعمل النهار؟\",\n",
    "\n",
    "        # Technology\n",
    "        \"شنو هو الذكاء الاصطناعي؟\",\n",
    "        \"كيفاش نستعمل الهاتف الذكي؟\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n🧪 Testing Darija Q&A Model:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        # Format as chat\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = trainer.processing_class.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        try:\n",
    "            response = darija_pipe(\n",
    "                formatted_prompt,\n",
    "                max_new_tokens=120,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=trainer.processing_class.eos_token_id\n",
    "            )\n",
    "\n",
    "            generated = response[0]['generated_text'][len(formatted_prompt):].strip()\n",
    "\n",
    "            print(f\"\\n{i}. 🙋‍♂️ {prompt}\")\n",
    "            print(f\"   🤖 {generated}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating response for '{prompt}': {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"🇲🇦 Moroccan Darija SmolLM2 Fine-tuning with Q&A Dataset\")\n",
    "    print(\"📚 Using Lyte/Moroccan-Darija-QA - High Quality Dataset\")\n",
    "    print(\"🎯 2000+ Question-Answer pairs covering 10 categories\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_darija_qa_smollm()\n",
    "\n",
    "    if trainer:\n",
    "        # Test Q&A capabilities\n",
    "        test_darija_qa_model(trainer)\n",
    "\n",
    "        # Save the model\n",
    "        save_path = \"smollm_darija_qa_finetuned\"\n",
    "        trainer.model.save_pretrained(save_path)\n",
    "        trainer.processing_class.save_pretrained(save_path)\n",
    "\n",
    "        print(f\"\\n✅ Darija Q&A fine-tuning complete!\")\n",
    "        print(f\"📁 Model saved to: {save_path}\")\n",
    "        print(f\"🇲🇦 Your SmolLM2 now speaks Moroccan Darija with Q&A capabilities!\")\n",
    "        print(f\"📊 Trained on {len(trainer.train_dataset)} examples\")\n",
    "\n",
    "        # Model usage instructions\n",
    "        print(\"\\n📖 Usage Instructions:\")\n",
    "        print(\"from transformers import pipeline\")\n",
    "        print(f\"pipe = pipeline('text-generation', model='{save_path}')\")\n",
    "        print(\"response = pipe('شنو هوما فوائد الرياضة؟')\")\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Training failed. Please check the error messages above.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "38769582455b488fada1f780855b7292",
      "65a054f88bb24d4e8f34a84021819ca0",
      "c599694d2fdf41e7936003522c485e52",
      "89b877c026cf43e78889b271f75960fc",
      "484e1cc126b0452d8d524c3228dea10a",
      "f3efe4f4cf2247e480b3b955094b531a",
      "57e374e6d7c14fbaa7b6f7e3bec41b48",
      "851f6c816ae6474381038349873bb562",
      "e722a70527c74cca82ead600c14e0c2e",
      "194682d360cf4dc2bcfb783812f8d3fe",
      "9da788e36f974c18bb6727863c1d5b46",
      "e6c03ec3e6554e119333da17939b1c16",
      "a13f81f0664d42648d1b4f8b1060cf09",
      "bcbadeafc4d74f3e94c6bc5dca944517",
      "6b56291335c943a3aaa4a393e0f8df1f",
      "4bc6c404f1c94aa2b6791a2c51f47381",
      "e26ac518c1a74f66a768d66b75e563ef",
      "93f2949066364b57a771bac7421226ac",
      "97bf57ce589a42d280b18aedb69845b3",
      "8aa97e02aba4474cb29a46f70ca25fda",
      "d05819cd9a414f6a849990f4d1f5ddfa",
      "29680f36c8464023a9e2fa3fec7aa784"
     ]
    },
    "id": "M72kZtDqrgzf",
    "outputId": "dfcc17d1-d7eb-49cc-cd9c-f82a16d967fb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🇲🇦 Moroccan Darija SmolLM2 Fine-tuning with Q&A Dataset\n",
      "📚 Using Lyte/Moroccan-Darija-QA - High Quality Dataset\n",
      "🎯 2000+ Question-Answer pairs covering 10 categories\n",
      "======================================================================\n",
      "🇲🇦 Starting Moroccan Darija Q&A Fine-tuning for SmolLM2-135M\n",
      "======================================================================\n",
      "\n",
      "1️⃣ Loading base model...\n",
      "✅ Model loaded: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "✅ Vocabulary size: 49152\n",
      "\n",
      "2️⃣ Preparing Darija Q&A datasets...\n",
      "🇲🇦 Loading Moroccan Darija Q&A Dataset...\n",
      "✅ Default config loaded: 2026 examples\n",
      "✅ Translated config loaded: 1300 examples\n",
      "✅ Reasoning config loaded: 144 examples\n",
      "\n",
      "📊 Dataset Preview:\n",
      "Question: واش التجارة مربحة فالمغرب؟\n",
      "Answer: التجارة مربحة ملي كتعرف السوق مزيان وكتختار المنتج المناسب. ولكن كتحتاج رأس مال وخبرة. أحسن شي تبدا ...\n",
      "Category: business\n",
      "--------------------------------------------------\n",
      "Question: كيفاش نفتح مقاولة صغيرة؟\n",
      "Answer: باش تفتح مقاولة صغيرة، مش للسجل التجاري وسجل فالضرائب. خود رخصة من البلدية ملي كانت حاجة تحتاج ليها....\n",
      "Category: business\n",
      "--------------------------------------------------\n",
      "Question: شحال كيكلف التسجيل فالسجل التجاري؟\n",
      "Answer: التسجيل فالسجل التجاري ما غاليش، تقريبا 300-500 درهم. ولكن كتحتاج وثائق: بطاقة التعريف، عقد الكراء أ...\n",
      "Category: business\n",
      "--------------------------------------------------\n",
      "\n",
      "🔄 Processing default Q&A dataset...\n",
      "🧠 Processing reasoning Q&A dataset...\n",
      "🌍 Adding translated examples for variety...\n",
      "✅ Total training examples prepared: 2370\n",
      "\n",
      "📈 Category Distribution:\n",
      "  business: 197 examples\n",
      "  culture: 274 examples\n",
      "  culture_reasoning: 63 examples\n",
      "  daily_life: 147 examples\n",
      "  education: 172 examples\n",
      "  food: 202 examples\n",
      "  health: 220 examples\n",
      "  religion: 194 examples\n",
      "  sports: 187 examples\n",
      "  technology: 259 examples\n",
      "  technology_reasoning: 81 examples\n",
      "  translated_translated: 200 examples\n",
      "  travel: 174 examples\n",
      "➕ Added 8 conversational examples\n",
      "\n",
      "🎯 Final dataset size: 2378 examples\n",
      "\n",
      "3️⃣ Tokenizing data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2378 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38769582455b488fada1f780855b7292"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Tokenization complete. Dataset size: 2378\n",
      "\n",
      "4️⃣ Setting up training...\n",
      "🚀 Available GPU memory: 15.8GB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2378 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6c03ec3e6554e119333da17939b1c16"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "5️⃣ Starting training...\n",
      "🕐 Training will take approximately 25-35 minutes on Colab T4\n",
      "📚 Learning from 2000+ Darija Q&A pairs...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 11:55, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.792800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.753100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Training completed!\n",
      "\n",
      "🧪 Testing Darija Q&A Model:\n",
      "==================================================\n",
      "\n",
      "1. 🙋‍♂️ واش التجارة مربحة فالمغرب؟\n",
      "   🤖 التجارة كيحتاج بالمغرب، كيمكن يحتاج العادات المغربية ولا ديال كاملة والمواقع. إذا كانت فالمغرب ماتجاه من الأنت.\n",
      "----------------------------------------\n",
      "\n",
      "2. 🙋‍♂️ كيفاش نفتح مقاولة صغيرة؟\n",
      "   🤖 المقاولة صغيرة من المغرب التغييرية. تطبيق مع المغرب التغييرية والتصلي. كيخلي فالمغرب التغييرية والمغرب العرفية. وماكيف مع المغرب التغ\n",
      "----------------------------------------\n",
      "\n",
      "3. 🙋‍♂️ شنو هوما فوائد الرياضة؟\n",
      "   🤖 الأساسية كتعلم بالطبخ المغربية المغربية والتواصل المغربية. كذلك الرياضة كتسعو بالطبخ المغربية، والمغرب كتعلم بالطبخ المغربية الم\n",
      "----------------------------------------\n",
      "\n",
      "4. 🙋‍♂️ كيفاش نحافظ على صحتي؟\n",
      "   🤖 بدات التجارة تحافظ على صحتي، خدمة كتجارة تجارة تأكد معها حيت خدمة وضعيف، واخر المعاهدات.\n",
      "----------------------------------------\n",
      "\n",
      "5. 🙋‍♂️ كيفاش نطيب الطاجين؟\n",
      "   🤖 الطاجين كيستعمل الجداد، ولكن من نقدر مزيان. الطاجين كيعطي كيستعمل الأكثرات والجداد، ولكن أنها تقدر من الماء والعمر. كيستعمل الج\n",
      "----------------------------------------\n",
      "\n",
      "6. 🙋‍♂️ شنو أحسن ماكلة مغريبية؟\n",
      "   🤖 المغريبية ماتطلب وماكل الصابحة، والخارج والمعاهدة. هادشي أحسن خارج تطلب حضر أكثر وأكثر تختلف أو الخطر.\n",
      "----------------------------------------\n",
      "\n",
      "7. 🙋‍♂️ شنو هوما التقاليد المغريبية؟\n",
      "   🤖 التقاليد المغريبية كتعلم الألفاء والمناسبات مزيانية. الجيد التقاليد من حيت تتخلص الألفاء. الطولوجي المغريبية حسب مربعة. العمل المغريبي\n",
      "----------------------------------------\n",
      "\n",
      "8. 🙋‍♂️ علاش عيد الفطر مهم؟\n",
      "   🤖 علاش عيد الفطر مهم كتخصص الشركات والسعرة. هادا تخصص مزيان كتتطلب الألوم والمحل المهم. كذلك كتطلب الشيطان والمعروف المهم. ثانيا ك\n",
      "----------------------------------------\n",
      "\n",
      "9. 🙋‍♂️ كيف داير؟\n",
      "   🤖 أيه، مقالات شوف شوف مقرات وقت فالرياضة، حاجة أيه، داودة تصنيقة حاجتاء فالمغرب، ومن فالمغرب كيعلمو فالطبيب.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "10. 🙋‍♂️ شنو كتعمل النهار؟\n",
      "   🤖 النهار، ديال الصفة تحديد التقليد. الشعب الإصلي، كيجب عندو الأرخاصة والتعليم. الأصلي الثقافي المغربي.\n",
      "----------------------------------------\n",
      "\n",
      "11. 🙋‍♂️ شنو هو الذكاء الاصطناعي؟\n",
      "   🤖 الذكاء الاصطناعي كتأثر معمولة المغرب والتساعد، والشيخ كتأثر تأثر عندها حيت تجنب جنوب تأثر كل مجيء.\n",
      "----------------------------------------\n",
      "\n",
      "12. 🙋‍♂️ كيفاش نستعمل الهاتف الذكي؟\n",
      "   🤖 الهاتف الذكي كتعرف فيها الطبقة الذكية. كيستعمل شي تقدر وضعفات حسب الشباب. كتخلص المغرب والحضرة. كتتغير على طبقة إلى طبقة تق\n",
      "----------------------------------------\n",
      "\n",
      "✅ Darija Q&A fine-tuning complete!\n",
      "📁 Model saved to: smollm_darija_qa_finetuned\n",
      "🇲🇦 Your SmolLM2 now speaks Moroccan Darija with Q&A capabilities!\n",
      "📊 Trained on 2378 examples\n",
      "\n",
      "📖 Usage Instructions:\n",
      "from transformers import pipeline\n",
      "pipe = pipeline('text-generation', model='smollm_darija_qa_finetuned')\n",
      "response = pipe('شنو هوما فوائد الرياضة؟')\n"
     ]
    }
   ]
  }
 ]
}

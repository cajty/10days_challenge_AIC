{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "38769582455b488fada1f780855b7292": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65a054f88bb24d4e8f34a84021819ca0",
       "IPY_MODEL_c599694d2fdf41e7936003522c485e52",
       "IPY_MODEL_89b877c026cf43e78889b271f75960fc"
      ],
      "layout": "IPY_MODEL_484e1cc126b0452d8d524c3228dea10a"
     }
    },
    "65a054f88bb24d4e8f34a84021819ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3efe4f4cf2247e480b3b955094b531a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_57e374e6d7c14fbaa7b6f7e3bec41b48",
      "value": "Map:â€‡100%"
     }
    },
    "c599694d2fdf41e7936003522c485e52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_851f6c816ae6474381038349873bb562",
      "max": 2378,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e722a70527c74cca82ead600c14e0c2e",
      "value": 2378
     }
    },
    "89b877c026cf43e78889b271f75960fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_194682d360cf4dc2bcfb783812f8d3fe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9da788e36f974c18bb6727863c1d5b46",
      "value": "â€‡2378/2378â€‡[00:01&lt;00:00,â€‡1673.68â€‡examples/s]"
     }
    },
    "484e1cc126b0452d8d524c3228dea10a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3efe4f4cf2247e480b3b955094b531a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57e374e6d7c14fbaa7b6f7e3bec41b48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "851f6c816ae6474381038349873bb562": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e722a70527c74cca82ead600c14e0c2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "194682d360cf4dc2bcfb783812f8d3fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da788e36f974c18bb6727863c1d5b46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6c03ec3e6554e119333da17939b1c16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a13f81f0664d42648d1b4f8b1060cf09",
       "IPY_MODEL_bcbadeafc4d74f3e94c6bc5dca944517",
       "IPY_MODEL_6b56291335c943a3aaa4a393e0f8df1f"
      ],
      "layout": "IPY_MODEL_4bc6c404f1c94aa2b6791a2c51f47381"
     }
    },
    "a13f81f0664d42648d1b4f8b1060cf09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e26ac518c1a74f66a768d66b75e563ef",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_93f2949066364b57a771bac7421226ac",
      "value": "Truncatingâ€‡trainâ€‡dataset:â€‡100%"
     }
    },
    "bcbadeafc4d74f3e94c6bc5dca944517": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97bf57ce589a42d280b18aedb69845b3",
      "max": 2378,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8aa97e02aba4474cb29a46f70ca25fda",
      "value": 2378
     }
    },
    "6b56291335c943a3aaa4a393e0f8df1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d05819cd9a414f6a849990f4d1f5ddfa",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_29680f36c8464023a9e2fa3fec7aa784",
      "value": "â€‡2378/2378â€‡[00:00&lt;00:00,â€‡80133.49â€‡examples/s]"
     }
    },
    "4bc6c404f1c94aa2b6791a2c51f47381": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e26ac518c1a74f66a768d66b75e563ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93f2949066364b57a771bac7421226ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97bf57ce589a42d280b18aedb69845b3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aa97e02aba4474cb29a46f70ca25fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d05819cd9a414f6a849990f4d1f5ddfa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29680f36c8464023a9e2fa3fec7aa784": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Moroccan Darija SmolLM2 Fine-tuning\n",
    "\n",
    "A comprehensive fine-tuning implementation that adapts **SmolLM2-135M-Instruct** to understand and generate Moroccan Darija (Arabic dialect) using high-quality Q&A datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project fine-tunes the `HuggingFaceTB/SmolLM2-135M-Instruct` model on over **2,300 Moroccan Darija question-answer pairs** from the `Lyte/Moroccan-Darija-QA` dataset. The model learns to respond to questions in Darija across multiple domains, including business, culture, health, food, technology, and daily conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kKAUzEilylH",
    "outputId": "8f89142f-b82d-42c4-9f0a-7083eacc94c0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mon Sep 15 13:32:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Moroccan Darija Fine-tuning for SmolLM2-135M using Lyte/Moroccan-Darija-QA\n",
    "# Optimized for Google Colab with high-quality Q&A dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# MOROCCAN DARIJA Q&A DATASET INTEGRATION\n",
    "# ============================================================================\n",
    "\n",
    "def load_moroccan_darija_qa_dataset():\n",
    "    \"\"\"Load the high-quality Moroccan Darija Q&A dataset from Lyte\"\"\"\n",
    "\n",
    "    print(\"ðŸ‡²ðŸ‡¦ Loading Moroccan Darija Q&A Dataset...\")\n",
    "\n",
    "    try:\n",
    "        # Load different configurations\n",
    "        dataset_default = load_dataset(\"Lyte/Moroccan-Darija-QA\", name=\"default\")\n",
    "        dataset_translated = load_dataset(\"Lyte/Moroccan-Darija-QA\", name=\"translated\")\n",
    "        dataset_reasoning = load_dataset(\"Lyte/Moroccan-Darija-QA\", name=\"reasoning\")\n",
    "\n",
    "        print(f\"âœ… Default config loaded: {len(dataset_default['train'])} examples\")\n",
    "        print(f\"âœ… Translated config loaded: {len(dataset_translated['train'])} examples\")\n",
    "        print(f\"âœ… Reasoning config loaded: {len(dataset_reasoning['train'])} examples\")\n",
    "\n",
    "        # Preview the data structure\n",
    "        print(\"\\nðŸ“Š Dataset Preview:\")\n",
    "        for i, example in enumerate(dataset_default['train']):\n",
    "            if i < 3:\n",
    "                print(f\"Question: {example['question']}\")\n",
    "                print(f\"Answer: {example['answer'][:100]}...\")\n",
    "                print(f\"Category: {example['category']}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "        return {\n",
    "            'default': dataset_default,\n",
    "            'translated': dataset_translated,\n",
    "            'reasoning': dataset_reasoning\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Q&A dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_darija_qa_for_training(qa_datasets):\n",
    "    \"\"\"Convert Q&A datasets to instruction-following format for training\"\"\"\n",
    "\n",
    "    if qa_datasets is None:\n",
    "        return None\n",
    "\n",
    "    training_examples = []\n",
    "\n",
    "    # Process default configuration (main dataset)\n",
    "    print(\"\\nðŸ”„ Processing default Q&A dataset...\")\n",
    "    default_data = qa_datasets['default']['train']\n",
    "\n",
    "    for example in default_data:\n",
    "        question = example['question'].strip()\n",
    "        answer = example['answer'].strip()\n",
    "        category = example['category']\n",
    "\n",
    "        # Skip very short answers\n",
    "        if len(answer) < 20:\n",
    "            continue\n",
    "\n",
    "        # Create instruction format\n",
    "        training_examples.append({\n",
    "            'prompt': question,\n",
    "            'response': answer,\n",
    "            'category': category\n",
    "        })\n",
    "\n",
    "    # Process reasoning configuration (adds thinking process)\n",
    "    print(\"ðŸ§  Processing reasoning Q&A dataset...\")\n",
    "    reasoning_data = qa_datasets['reasoning']['train']\n",
    "\n",
    "    for example in reasoning_data:\n",
    "        question = example['question'].strip()\n",
    "        answer = example['answer'].strip()\n",
    "        category = example['category']\n",
    "\n",
    "        # Skip very short answers\n",
    "        if len(answer) < 20:\n",
    "            continue\n",
    "\n",
    "        training_examples.append({\n",
    "            'prompt': question,\n",
    "            'response': answer,\n",
    "            'category': f\"{category}_reasoning\"\n",
    "        })\n",
    "\n",
    "    # Optionally include some translated examples for diversity\n",
    "    print(\"ðŸŒ Adding translated examples for variety...\")\n",
    "    translated_data = qa_datasets['translated']['train']\n",
    "\n",
    "    # Take a sample of translated data (not all to maintain Darija focus)\n",
    "    sample_size = min(200, len(translated_data))\n",
    "    translated_sample = random.sample(list(translated_data), sample_size)\n",
    "\n",
    "    for example in translated_sample:\n",
    "        question = example['question'].strip()\n",
    "        answer = example['answer'].strip()\n",
    "        category = example['category']\n",
    "\n",
    "        training_examples.append({\n",
    "            'prompt': question,\n",
    "            'response': answer,\n",
    "            'category': f\"{category}_translated\"\n",
    "        })\n",
    "\n",
    "    print(f\"âœ… Total training examples prepared: {len(training_examples)}\")\n",
    "\n",
    "    # Show category distribution\n",
    "    categories = {}\n",
    "    for ex in training_examples:\n",
    "        cat = ex['category']\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Category Distribution:\")\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        print(f\"  {cat}: {count} examples\")\n",
    "\n",
    "    return Dataset.from_list(training_examples)\n",
    "\n",
    "def add_conversational_examples():\n",
    "    \"\"\"Add some basic conversational examples to complement the Q&A data\"\"\"\n",
    "\n",
    "    darija_conversations = [\n",
    "        {\n",
    "            \"prompt\": \"ÙƒÙŠÙ Ø¯Ø§ÙŠØ±ØŸ\",\n",
    "            \"response\": \"Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ØŒ Ø¨Ø®ÙŠØ±. Ø£Ù†Øª ÙƒÙŠÙ Ø¯Ø§ÙŠØ±ØŸ\",\n",
    "            \"category\": \"greeting\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Ø´Ù†Ùˆ ÙƒØªØ¹Ù…Ù„ Ø§Ù„ÙŠÙˆÙ…ØŸ\",\n",
    "            \"response\": \"ÙƒÙ†Ø®Ø¯Ù… ÙØ§Ù„Ø¨ÙŠØª ÙˆÙƒÙ†Ù‚Ø±Ø§ Ø´ÙˆÙŠØ©. Ø£Ù†Øª Ø´Ù†Ùˆ ÙƒØªØ¹Ù…Ù„ØŸ\",\n",
    "            \"category\": \"daily_conversation\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"ÙˆØ§Ø´ Ø¨ØºÙŠØªÙŠ ØªØ´Ø±Ø¨ Ø£ØªØ§ÙŠØŸ\",\n",
    "            \"response\": \"Ø¥ÙŠÙ‡ Ø¨ØºÙŠØªØŒ Ø´ÙƒØ±Ø§ Ù„ÙŠÙƒ. Ø£ØªØ§ÙŠ Ø¨Ø§Ù„Ù†Ø¹Ù†Ø§Ø¹ Ø²ÙŠÙ† Ø¨Ø²Ø§Ù.\",\n",
    "            \"category\": \"hospitality\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Ø´ÙƒØ±Ø§ Ù„ÙŠÙƒ Ø¨Ø²Ø§Ù\",\n",
    "            \"response\": \"Ø§Ù„Ø¹ÙÙˆØŒ Ù…Ø§Ø´ÙŠ Ù…Ø´ÙƒÙ„. ÙƒÙ„Ø´ÙŠ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø®ÙŠØ±.\",\n",
    "            \"category\": \"politeness\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Ø¨ØµØ­ØªÙƒ\",\n",
    "            \"response\": \"Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„ØµØ­Ø©. Ø´ÙƒØ±Ø§ Ù„ÙŠÙƒ.\",\n",
    "            \"category\": \"well_wishes\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Ø£Ø´Ù†Ùˆ ÙƒØªØ­Ø¨ ÙØ§Ù„Ù…Ø§ÙƒÙ„Ø© Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠØ©ØŸ\",\n",
    "            \"response\": \"ÙƒÙ†Ø­Ø¨ Ø§Ù„Ø·Ø§Ø¬ÙŠÙ† ÙˆØ§Ù„ÙƒØ³ÙƒØ³ ÙˆØ§Ù„Ù¾Ø§Ø³ØªÙŠØ·Ø§. ÙˆØ§Ù„Ø­Ù„ÙˆÙŠØ§Øª Ù…ØºØ±ÙŠØ¨ÙŠØ© Ø²ÙŠÙ†ÙŠÙ† Ø¨Ø²Ø§Ù.\",\n",
    "            \"category\": \"food_conversation\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"ÙˆØ§Ø´ ÙƒØªØ¹Ø±Ù ØªØ·ÙŠØ¨ØŸ\",\n",
    "            \"response\": \"Ø¥ÙŠÙ‡ ÙƒÙ†Ø¹Ø±Ù Ù†Ø·ÙŠØ¨ Ø´ÙˆÙŠØ©. ÙƒÙ†Ø·ÙŠØ¨ Ø§Ù„Ø·Ø§Ø¬ÙŠÙ† ÙˆØ§Ù„Ø­Ø±ÙŠØ±Ø© Ù…Ø²ÙŠØ§Ù†.\",\n",
    "            \"category\": \"cooking\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…\",\n",
    "            \"response\": \"ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡. Ø£Ù‡Ù„Ø§ ÙˆØ³Ù‡Ù„Ø§.\",\n",
    "            \"category\": \"religious_greeting\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"âž• Added {len(darija_conversations)} conversational examples\")\n",
    "    return Dataset.from_list(darija_conversations)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED DATASET PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_complete_darija_dataset():\n",
    "    \"\"\"Combine Q&A dataset with conversational examples\"\"\"\n",
    "\n",
    "    # Load the main Q&A dataset\n",
    "    qa_datasets = load_moroccan_darija_qa_dataset()\n",
    "    if qa_datasets is None:\n",
    "        print(\"âŒ Failed to load Q&A dataset\")\n",
    "        return None\n",
    "\n",
    "    # Prepare Q&A data for training\n",
    "    qa_training_data = prepare_darija_qa_for_training(qa_datasets)\n",
    "\n",
    "    # Add conversational examples\n",
    "    conversation_data = add_conversational_examples()\n",
    "\n",
    "    # Combine datasets\n",
    "    if qa_training_data and conversation_data:\n",
    "        combined_dataset = concatenate_datasets([qa_training_data, conversation_data])\n",
    "    else:\n",
    "        combined_dataset = qa_training_data or conversation_data\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Final dataset size: {len(combined_dataset)} examples\")\n",
    "    return combined_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETUP FOR DARIJA Q&A\n",
    "# ============================================================================\n",
    "\n",
    "def setup_darija_qa_training():\n",
    "    \"\"\"Setup training configuration optimized for Darija Q&A and Colab\"\"\"\n",
    "\n",
    "    # Check GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"ðŸš€ Available GPU memory: {gpu_memory:.1f}GB\")\n",
    "\n",
    "        # Adjust batch size based on memory\n",
    "        if gpu_memory < 16:  # T4 in Colab\n",
    "            batch_size = 1\n",
    "            grad_accumulation = 8\n",
    "        else:\n",
    "            batch_size = 2\n",
    "            grad_accumulation = 4\n",
    "    else:\n",
    "        print(\"âš ï¸  Running on CPU - training will be slower\")\n",
    "        batch_size = 1\n",
    "        grad_accumulation = 8\n",
    "\n",
    "    # Adjusted for larger, higher-quality dataset\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=grad_accumulation,\n",
    "        warmup_steps=100,  # More warmup for larger dataset\n",
    "        max_steps=500,     # More steps for comprehensive Q&A learning\n",
    "        learning_rate=1e-5,  # Lower LR for stability with quality data\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=42,\n",
    "        output_dir=\"darija_qa_model_outputs\",\n",
    "        save_steps=150,\n",
    "        dataloader_drop_last=True,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    return training_args\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED TOKENIZATION FOR Q&A FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_darija_qa_tokenization(dataset, tokenizer, max_length):\n",
    "    \"\"\"Prepare tokenization specifically for Darija Q&A format\"\"\"\n",
    "\n",
    "    def tokenize_qa_examples(examples):\n",
    "        \"\"\"Tokenize Q&A examples with proper chat format\"\"\"\n",
    "\n",
    "        texts = []\n",
    "        for prompt, response in zip(examples[\"prompt\"], examples[\"response\"]):\n",
    "            # Create chat format for Q&A\n",
    "            conversation = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": response}\n",
    "            ]\n",
    "\n",
    "            # Apply chat template\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                conversation,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "\n",
    "        # Tokenize with appropriate settings for Q&A\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_qa_examples,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_darija_qa_smollm():\n",
    "    \"\"\"Main function to train SmolLM2 on high-quality Darija Q&A data\"\"\"\n",
    "\n",
    "    print(\"ðŸ‡²ðŸ‡¦ Starting Moroccan Darija Q&A Fine-tuning for SmolLM2-135M\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # 1. Load model and tokenizer\n",
    "    print(\"\\n1ï¸âƒ£ Loading base model...\")\n",
    "    model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(f\"âœ… Model loaded: {model_name}\")\n",
    "    print(f\"âœ… Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "    # 2. Prepare dataset\n",
    "    print(\"\\n2ï¸âƒ£ Preparing Darija Q&A datasets...\")\n",
    "    darija_dataset = prepare_complete_darija_dataset()\n",
    "\n",
    "    if darija_dataset is None:\n",
    "        print(\"âŒ Failed to prepare dataset\")\n",
    "        return None\n",
    "\n",
    "    # 3. Tokenize dataset\n",
    "    print(\"\\n3ï¸âƒ£ Tokenizing data...\")\n",
    "    max_seq_length = 512  # Increased for Q&A format\n",
    "    tokenized_dataset = prepare_darija_qa_tokenization(darija_dataset, tokenizer, max_seq_length)\n",
    "\n",
    "    print(f\"âœ… Tokenization complete. Dataset size: {len(tokenized_dataset)}\")\n",
    "\n",
    "    # 4. Setup training\n",
    "    print(\"\\n4ï¸âƒ£ Setting up training...\")\n",
    "    training_args = setup_darija_qa_training()\n",
    "\n",
    "    # 5. Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # 6. Start training\n",
    "    print(\"\\n5ï¸âƒ£ Starting training...\")\n",
    "    print(\"ðŸ• Training will take approximately 25-35 minutes on Colab T4\")\n",
    "    print(\"ðŸ“š Learning from 2000+ Darija Q&A pairs...\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"âœ… Training completed!\")\n",
    "    return trainer\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED TESTING FOR Q&A MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def test_darija_qa_model(trainer):\n",
    "    \"\"\"Test the fine-tuned model on various Darija Q&A scenarios\"\"\"\n",
    "\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # Create pipeline\n",
    "    darija_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=trainer.model,\n",
    "        tokenizer=trainer.processing_class,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "\n",
    "    # Test prompts covering different categories from the dataset\n",
    "    test_prompts = [\n",
    "        # Business\n",
    "        \"ÙˆØ§Ø´ Ø§Ù„ØªØ¬Ø§Ø±Ø© Ù…Ø±Ø¨Ø­Ø© ÙØ§Ù„Ù…ØºØ±Ø¨ØŸ\",\n",
    "        \"ÙƒÙŠÙØ§Ø´ Ù†ÙØªØ­ Ù…Ù‚Ø§ÙˆÙ„Ø© ØµØºÙŠØ±Ø©ØŸ\",\n",
    "\n",
    "        # Health\n",
    "        \"Ø´Ù†Ùˆ Ù‡ÙˆÙ…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŸ\",\n",
    "        \"ÙƒÙŠÙØ§Ø´ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØµØ­ØªÙŠØŸ\",\n",
    "\n",
    "        # Food\n",
    "        \"ÙƒÙŠÙØ§Ø´ Ù†Ø·ÙŠØ¨ Ø§Ù„Ø·Ø§Ø¬ÙŠÙ†ØŸ\",\n",
    "        \"Ø´Ù†Ùˆ Ø£Ø­Ø³Ù† Ù…Ø§ÙƒÙ„Ø© Ù…ØºØ±ÙŠØ¨ÙŠØ©ØŸ\",\n",
    "\n",
    "        # Culture\n",
    "        \"Ø´Ù†Ùˆ Ù‡ÙˆÙ…Ø§ Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠØ©ØŸ\",\n",
    "        \"Ø¹Ù„Ø§Ø´ Ø¹ÙŠØ¯ Ø§Ù„ÙØ·Ø± Ù…Ù‡Ù…ØŸ\",\n",
    "\n",
    "        # Daily conversation\n",
    "        \"ÙƒÙŠÙ Ø¯Ø§ÙŠØ±ØŸ\",\n",
    "        \"Ø´Ù†Ùˆ ÙƒØªØ¹Ù…Ù„ Ø§Ù„Ù†Ù‡Ø§Ø±ØŸ\",\n",
    "\n",
    "        # Technology\n",
    "        \"Ø´Ù†Ùˆ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\",\n",
    "        \"ÙƒÙŠÙØ§Ø´ Ù†Ø³ØªØ¹Ù…Ù„ Ø§Ù„Ù‡Ø§ØªÙ Ø§Ù„Ø°ÙƒÙŠØŸ\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nðŸ§ª Testing Darija Q&A Model:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        # Format as chat\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = trainer.processing_class.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        try:\n",
    "            response = darija_pipe(\n",
    "                formatted_prompt,\n",
    "                max_new_tokens=120,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=trainer.processing_class.eos_token_id\n",
    "            )\n",
    "\n",
    "            generated = response[0]['generated_text'][len(formatted_prompt):].strip()\n",
    "\n",
    "            print(f\"\\n{i}. ðŸ™‹â€â™‚ï¸ {prompt}\")\n",
    "            print(f\"   ðŸ¤– {generated}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error generating response for '{prompt}': {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"ðŸ‡²ðŸ‡¦ Moroccan Darija SmolLM2 Fine-tuning with Q&A Dataset\")\n",
    "    print(\"ðŸ“š Using Lyte/Moroccan-Darija-QA - High Quality Dataset\")\n",
    "    print(\"ðŸŽ¯ 2000+ Question-Answer pairs covering 10 categories\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Train the model\n",
    "    trainer = train_darija_qa_smollm()\n",
    "\n",
    "    if trainer:\n",
    "        # Test Q&A capabilities\n",
    "        test_darija_qa_model(trainer)\n",
    "\n",
    "        # Save the model\n",
    "        save_path = \"smollm_darija_qa_finetuned\"\n",
    "        trainer.model.save_pretrained(save_path)\n",
    "        trainer.processing_class.save_pretrained(save_path)\n",
    "\n",
    "        print(f\"\\nâœ… Darija Q&A fine-tuning complete!\")\n",
    "        print(f\"ðŸ“ Model saved to: {save_path}\")\n",
    "        print(f\"ðŸ‡²ðŸ‡¦ Your SmolLM2 now speaks Moroccan Darija with Q&A capabilities!\")\n",
    "        print(f\"ðŸ“Š Trained on {len(trainer.train_dataset)} examples\")\n",
    "\n",
    "        # Model usage instructions\n",
    "        print(\"\\nðŸ“– Usage Instructions:\")\n",
    "        print(\"from transformers import pipeline\")\n",
    "        print(f\"pipe = pipeline('text-generation', model='{save_path}')\")\n",
    "        print(\"response = pipe('Ø´Ù†Ùˆ Ù‡ÙˆÙ…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŸ')\")\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ Training failed. Please check the error messages above.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "38769582455b488fada1f780855b7292",
      "65a054f88bb24d4e8f34a84021819ca0",
      "c599694d2fdf41e7936003522c485e52",
      "89b877c026cf43e78889b271f75960fc",
      "484e1cc126b0452d8d524c3228dea10a",
      "f3efe4f4cf2247e480b3b955094b531a",
      "57e374e6d7c14fbaa7b6f7e3bec41b48",
      "851f6c816ae6474381038349873bb562",
      "e722a70527c74cca82ead600c14e0c2e",
      "194682d360cf4dc2bcfb783812f8d3fe",
      "9da788e36f974c18bb6727863c1d5b46",
      "e6c03ec3e6554e119333da17939b1c16",
      "a13f81f0664d42648d1b4f8b1060cf09",
      "bcbadeafc4d74f3e94c6bc5dca944517",
      "6b56291335c943a3aaa4a393e0f8df1f",
      "4bc6c404f1c94aa2b6791a2c51f47381",
      "e26ac518c1a74f66a768d66b75e563ef",
      "93f2949066364b57a771bac7421226ac",
      "97bf57ce589a42d280b18aedb69845b3",
      "8aa97e02aba4474cb29a46f70ca25fda",
      "d05819cd9a414f6a849990f4d1f5ddfa",
      "29680f36c8464023a9e2fa3fec7aa784"
     ]
    },
    "id": "M72kZtDqrgzf",
    "outputId": "dfcc17d1-d7eb-49cc-cd9c-f82a16d967fb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ðŸ‡²ðŸ‡¦ Moroccan Darija SmolLM2 Fine-tuning with Q&A Dataset\n",
      "ðŸ“š Using Lyte/Moroccan-Darija-QA - High Quality Dataset\n",
      "ðŸŽ¯ 2000+ Question-Answer pairs covering 10 categories\n",
      "======================================================================\n",
      "ðŸ‡²ðŸ‡¦ Starting Moroccan Darija Q&A Fine-tuning for SmolLM2-135M\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£ Loading base model...\n",
      "âœ… Model loaded: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "âœ… Vocabulary size: 49152\n",
      "\n",
      "2ï¸âƒ£ Preparing Darija Q&A datasets...\n",
      "ðŸ‡²ðŸ‡¦ Loading Moroccan Darija Q&A Dataset...\n",
      "âœ… Default config loaded: 2026 examples\n",
      "âœ… Translated config loaded: 1300 examples\n",
      "âœ… Reasoning config loaded: 144 examples\n",
      "\n",
      "ðŸ“Š Dataset Preview:\n",
      "Question: ÙˆØ§Ø´ Ø§Ù„ØªØ¬Ø§Ø±Ø© Ù…Ø±Ø¨Ø­Ø© ÙØ§Ù„Ù…ØºØ±Ø¨ØŸ\n",
      "Answer: Ø§Ù„ØªØ¬Ø§Ø±Ø© Ù…Ø±Ø¨Ø­Ø© Ù…Ù„ÙŠ ÙƒØªØ¹Ø±Ù Ø§Ù„Ø³ÙˆÙ‚ Ù…Ø²ÙŠØ§Ù† ÙˆÙƒØªØ®ØªØ§Ø± Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨. ÙˆÙ„ÙƒÙ† ÙƒØªØ­ØªØ§Ø¬ Ø±Ø£Ø³ Ù…Ø§Ù„ ÙˆØ®Ø¨Ø±Ø©. Ø£Ø­Ø³Ù† Ø´ÙŠ ØªØ¨Ø¯Ø§ ...\n",
      "Category: business\n",
      "--------------------------------------------------\n",
      "Question: ÙƒÙŠÙØ§Ø´ Ù†ÙØªØ­ Ù…Ù‚Ø§ÙˆÙ„Ø© ØµØºÙŠØ±Ø©ØŸ\n",
      "Answer: Ø¨Ø§Ø´ ØªÙØªØ­ Ù…Ù‚Ø§ÙˆÙ„Ø© ØµØºÙŠØ±Ø©ØŒ Ù…Ø´ Ù„Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ ÙˆØ³Ø¬Ù„ ÙØ§Ù„Ø¶Ø±Ø§Ø¦Ø¨. Ø®ÙˆØ¯ Ø±Ø®ØµØ© Ù…Ù† Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© Ù…Ù„ÙŠ ÙƒØ§Ù†Øª Ø­Ø§Ø¬Ø© ØªØ­ØªØ§Ø¬ Ù„ÙŠÙ‡Ø§....\n",
      "Category: business\n",
      "--------------------------------------------------\n",
      "Question: Ø´Ø­Ø§Ù„ ÙƒÙŠÙƒÙ„Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙØ§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠØŸ\n",
      "Answer: Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙØ§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ¬Ø§Ø±ÙŠ Ù…Ø§ ØºØ§Ù„ÙŠØ´ØŒ ØªÙ‚Ø±ÙŠØ¨Ø§ 300-500 Ø¯Ø±Ù‡Ù…. ÙˆÙ„ÙƒÙ† ÙƒØªØ­ØªØ§Ø¬ ÙˆØ«Ø§Ø¦Ù‚: Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙŠÙØŒ Ø¹Ù‚Ø¯ Ø§Ù„ÙƒØ±Ø§Ø¡ Ø£...\n",
      "Category: business\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ”„ Processing default Q&A dataset...\n",
      "ðŸ§  Processing reasoning Q&A dataset...\n",
      "ðŸŒ Adding translated examples for variety...\n",
      "âœ… Total training examples prepared: 2370\n",
      "\n",
      "ðŸ“ˆ Category Distribution:\n",
      "  business: 197 examples\n",
      "  culture: 274 examples\n",
      "  culture_reasoning: 63 examples\n",
      "  daily_life: 147 examples\n",
      "  education: 172 examples\n",
      "  food: 202 examples\n",
      "  health: 220 examples\n",
      "  religion: 194 examples\n",
      "  sports: 187 examples\n",
      "  technology: 259 examples\n",
      "  technology_reasoning: 81 examples\n",
      "  translated_translated: 200 examples\n",
      "  travel: 174 examples\n",
      "âž• Added 8 conversational examples\n",
      "\n",
      "ðŸŽ¯ Final dataset size: 2378 examples\n",
      "\n",
      "3ï¸âƒ£ Tokenizing data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2378 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38769582455b488fada1f780855b7292"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Tokenization complete. Dataset size: 2378\n",
      "\n",
      "4ï¸âƒ£ Setting up training...\n",
      "ðŸš€ Available GPU memory: 15.8GB\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2378 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6c03ec3e6554e119333da17939b1c16"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "5ï¸âƒ£ Starting training...\n",
      "ðŸ• Training will take approximately 25-35 minutes on Colab T4\n",
      "ðŸ“š Learning from 2000+ Darija Q&A pairs...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 11:55, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.930500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.792800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.753100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Training completed!\n",
      "\n",
      "ðŸ§ª Testing Darija Q&A Model:\n",
      "==================================================\n",
      "\n",
      "1. ðŸ™‹â€â™‚ï¸ ÙˆØ§Ø´ Ø§Ù„ØªØ¬Ø§Ø±Ø© Ù…Ø±Ø¨Ø­Ø© ÙØ§Ù„Ù…ØºØ±Ø¨ØŸ\n",
      "   ðŸ¤– Ø§Ù„ØªØ¬Ø§Ø±Ø© ÙƒÙŠØ­ØªØ§Ø¬ Ø¨Ø§Ù„Ù…ØºØ±Ø¨ØŒ ÙƒÙŠÙ…ÙƒÙ† ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© ÙˆÙ„Ø§ Ø¯ÙŠØ§Ù„ ÙƒØ§Ù…Ù„Ø© ÙˆØ§Ù„Ù…ÙˆØ§Ù‚Ø¹. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ÙØ§Ù„Ù…ØºØ±Ø¨ Ù…Ø§ØªØ¬Ø§Ù‡ Ù…Ù† Ø§Ù„Ø£Ù†Øª.\n",
      "----------------------------------------\n",
      "\n",
      "2. ðŸ™‹â€â™‚ï¸ ÙƒÙŠÙØ§Ø´ Ù†ÙØªØ­ Ù…Ù‚Ø§ÙˆÙ„Ø© ØµØºÙŠØ±Ø©ØŸ\n",
      "   ðŸ¤– Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„Ø© ØµØºÙŠØ±Ø© Ù…Ù† Ø§Ù„Ù…ØºØ±Ø¨ Ø§Ù„ØªØºÙŠÙŠØ±ÙŠØ©. ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…ØºØ±Ø¨ Ø§Ù„ØªØºÙŠÙŠØ±ÙŠØ© ÙˆØ§Ù„ØªØµÙ„ÙŠ. ÙƒÙŠØ®Ù„ÙŠ ÙØ§Ù„Ù…ØºØ±Ø¨ Ø§Ù„ØªØºÙŠÙŠØ±ÙŠØ© ÙˆØ§Ù„Ù…ØºØ±Ø¨ Ø§Ù„Ø¹Ø±ÙÙŠØ©. ÙˆÙ…Ø§ÙƒÙŠÙ Ù…Ø¹ Ø§Ù„Ù…ØºØ±Ø¨ Ø§Ù„ØªØº\n",
      "----------------------------------------\n",
      "\n",
      "3. ðŸ™‹â€â™‚ï¸ Ø´Ù†Ùˆ Ù‡ÙˆÙ…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŸ\n",
      "   ðŸ¤– Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙƒØªØ¹Ù„Ù… Ø¨Ø§Ù„Ø·Ø¨Ø® Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© ÙˆØ§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©. ÙƒØ°Ù„Ùƒ Ø§Ù„Ø±ÙŠØ§Ø¶Ø© ÙƒØªØ³Ø¹Ùˆ Ø¨Ø§Ù„Ø·Ø¨Ø® Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©ØŒ ÙˆØ§Ù„Ù…ØºØ±Ø¨ ÙƒØªØ¹Ù„Ù… Ø¨Ø§Ù„Ø·Ø¨Ø® Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø§Ù„Ù…\n",
      "----------------------------------------\n",
      "\n",
      "4. ðŸ™‹â€â™‚ï¸ ÙƒÙŠÙØ§Ø´ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØµØ­ØªÙŠØŸ\n",
      "   ðŸ¤– Ø¨Ø¯Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±Ø© ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ØµØ­ØªÙŠØŒ Ø®Ø¯Ù…Ø© ÙƒØªØ¬Ø§Ø±Ø© ØªØ¬Ø§Ø±Ø© ØªØ£ÙƒØ¯ Ù…Ø¹Ù‡Ø§ Ø­ÙŠØª Ø®Ø¯Ù…Ø© ÙˆØ¶Ø¹ÙŠÙØŒ ÙˆØ§Ø®Ø± Ø§Ù„Ù…Ø¹Ø§Ù‡Ø¯Ø§Øª.\n",
      "----------------------------------------\n",
      "\n",
      "5. ðŸ™‹â€â™‚ï¸ ÙƒÙŠÙØ§Ø´ Ù†Ø·ÙŠØ¨ Ø§Ù„Ø·Ø§Ø¬ÙŠÙ†ØŸ\n",
      "   ðŸ¤– Ø§Ù„Ø·Ø§Ø¬ÙŠÙ† ÙƒÙŠØ³ØªØ¹Ù…Ù„ Ø§Ù„Ø¬Ø¯Ø§Ø¯ØŒ ÙˆÙ„ÙƒÙ† Ù…Ù† Ù†Ù‚Ø¯Ø± Ù…Ø²ÙŠØ§Ù†. Ø§Ù„Ø·Ø§Ø¬ÙŠÙ† ÙƒÙŠØ¹Ø·ÙŠ ÙƒÙŠØ³ØªØ¹Ù…Ù„ Ø§Ù„Ø£ÙƒØ«Ø±Ø§Øª ÙˆØ§Ù„Ø¬Ø¯Ø§Ø¯ØŒ ÙˆÙ„ÙƒÙ† Ø£Ù†Ù‡Ø§ ØªÙ‚Ø¯Ø± Ù…Ù† Ø§Ù„Ù…Ø§Ø¡ ÙˆØ§Ù„Ø¹Ù…Ø±. ÙƒÙŠØ³ØªØ¹Ù…Ù„ Ø§Ù„Ø¬\n",
      "----------------------------------------\n",
      "\n",
      "6. ðŸ™‹â€â™‚ï¸ Ø´Ù†Ùˆ Ø£Ø­Ø³Ù† Ù…Ø§ÙƒÙ„Ø© Ù…ØºØ±ÙŠØ¨ÙŠØ©ØŸ\n",
      "   ðŸ¤– Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠØ© Ù…Ø§ØªØ·Ù„Ø¨ ÙˆÙ…Ø§ÙƒÙ„ Ø§Ù„ØµØ§Ø¨Ø­Ø©ØŒ ÙˆØ§Ù„Ø®Ø§Ø±Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù‡Ø¯Ø©. Ù‡Ø§Ø¯Ø´ÙŠ Ø£Ø­Ø³Ù† Ø®Ø§Ø±Ø¬ ØªØ·Ù„Ø¨ Ø­Ø¶Ø± Ø£ÙƒØ«Ø± ÙˆØ£ÙƒØ«Ø± ØªØ®ØªÙ„Ù Ø£Ùˆ Ø§Ù„Ø®Ø·Ø±.\n",
      "----------------------------------------\n",
      "\n",
      "7. ðŸ™‹â€â™‚ï¸ Ø´Ù†Ùˆ Ù‡ÙˆÙ…Ø§ Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠØ©ØŸ\n",
      "   ðŸ¤– Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠØ© ÙƒØªØ¹Ù„Ù… Ø§Ù„Ø£Ù„ÙØ§Ø¡ ÙˆØ§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª Ù…Ø²ÙŠØ§Ù†ÙŠØ©. Ø§Ù„Ø¬ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ Ù…Ù† Ø­ÙŠØª ØªØªØ®Ù„Øµ Ø§Ù„Ø£Ù„ÙØ§Ø¡. Ø§Ù„Ø·ÙˆÙ„ÙˆØ¬ÙŠ Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠØ© Ø­Ø³Ø¨ Ù…Ø±Ø¨Ø¹Ø©. Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…ØºØ±ÙŠØ¨ÙŠ\n",
      "----------------------------------------\n",
      "\n",
      "8. ðŸ™‹â€â™‚ï¸ Ø¹Ù„Ø§Ø´ Ø¹ÙŠØ¯ Ø§Ù„ÙØ·Ø± Ù…Ù‡Ù…ØŸ\n",
      "   ðŸ¤– Ø¹Ù„Ø§Ø´ Ø¹ÙŠØ¯ Ø§Ù„ÙØ·Ø± Ù…Ù‡Ù… ÙƒØªØ®ØµØµ Ø§Ù„Ø´Ø±ÙƒØ§Øª ÙˆØ§Ù„Ø³Ø¹Ø±Ø©. Ù‡Ø§Ø¯Ø§ ØªØ®ØµØµ Ù…Ø²ÙŠØ§Ù† ÙƒØªØªØ·Ù„Ø¨ Ø§Ù„Ø£Ù„ÙˆÙ… ÙˆØ§Ù„Ù…Ø­Ù„ Ø§Ù„Ù…Ù‡Ù…. ÙƒØ°Ù„Ùƒ ÙƒØªØ·Ù„Ø¨ Ø§Ù„Ø´ÙŠØ·Ø§Ù† ÙˆØ§Ù„Ù…Ø¹Ø±ÙˆÙ Ø§Ù„Ù…Ù‡Ù…. Ø«Ø§Ù†ÙŠØ§ Ùƒ\n",
      "----------------------------------------\n",
      "\n",
      "9. ðŸ™‹â€â™‚ï¸ ÙƒÙŠÙ Ø¯Ø§ÙŠØ±ØŸ\n",
      "   ðŸ¤– Ø£ÙŠÙ‡ØŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø´ÙˆÙ Ø´ÙˆÙ Ù…Ù‚Ø±Ø§Øª ÙˆÙ‚Øª ÙØ§Ù„Ø±ÙŠØ§Ø¶Ø©ØŒ Ø­Ø§Ø¬Ø© Ø£ÙŠÙ‡ØŒ Ø¯Ø§ÙˆØ¯Ø© ØªØµÙ†ÙŠÙ‚Ø© Ø­Ø§Ø¬ØªØ§Ø¡ ÙØ§Ù„Ù…ØºØ±Ø¨ØŒ ÙˆÙ…Ù† ÙØ§Ù„Ù…ØºØ±Ø¨ ÙƒÙŠØ¹Ù„Ù…Ùˆ ÙØ§Ù„Ø·Ø¨ÙŠØ¨.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "10. ðŸ™‹â€â™‚ï¸ Ø´Ù†Ùˆ ÙƒØªØ¹Ù…Ù„ Ø§Ù„Ù†Ù‡Ø§Ø±ØŸ\n",
      "   ðŸ¤– Ø§Ù„Ù†Ù‡Ø§Ø±ØŒ Ø¯ÙŠØ§Ù„ Ø§Ù„ØµÙØ© ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯. Ø§Ù„Ø´Ø¹Ø¨ Ø§Ù„Ø¥ØµÙ„ÙŠØŒ ÙƒÙŠØ¬Ø¨ Ø¹Ù†Ø¯Ùˆ Ø§Ù„Ø£Ø±Ø®Ø§ØµØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ…. Ø§Ù„Ø£ØµÙ„ÙŠ Ø§Ù„Ø«Ù‚Ø§ÙÙŠ Ø§Ù„Ù…ØºØ±Ø¨ÙŠ.\n",
      "----------------------------------------\n",
      "\n",
      "11. ðŸ™‹â€â™‚ï¸ Ø´Ù†Ùˆ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\n",
      "   ðŸ¤– Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙƒØªØ£Ø«Ø± Ù…Ø¹Ù…ÙˆÙ„Ø© Ø§Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù„ØªØ³Ø§Ø¹Ø¯ØŒ ÙˆØ§Ù„Ø´ÙŠØ® ÙƒØªØ£Ø«Ø± ØªØ£Ø«Ø± Ø¹Ù†Ø¯Ù‡Ø§ Ø­ÙŠØª ØªØ¬Ù†Ø¨ Ø¬Ù†ÙˆØ¨ ØªØ£Ø«Ø± ÙƒÙ„ Ù…Ø¬ÙŠØ¡.\n",
      "----------------------------------------\n",
      "\n",
      "12. ðŸ™‹â€â™‚ï¸ ÙƒÙŠÙØ§Ø´ Ù†Ø³ØªØ¹Ù…Ù„ Ø§Ù„Ù‡Ø§ØªÙ Ø§Ù„Ø°ÙƒÙŠØŸ\n",
      "   ðŸ¤– Ø§Ù„Ù‡Ø§ØªÙ Ø§Ù„Ø°ÙƒÙŠ ÙƒØªØ¹Ø±Ù ÙÙŠÙ‡Ø§ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø°ÙƒÙŠØ©. ÙƒÙŠØ³ØªØ¹Ù…Ù„ Ø´ÙŠ ØªÙ‚Ø¯Ø± ÙˆØ¶Ø¹ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø´Ø¨Ø§Ø¨. ÙƒØªØ®Ù„Øµ Ø§Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù„Ø­Ø¶Ø±Ø©. ÙƒØªØªØºÙŠØ± Ø¹Ù„Ù‰ Ø·Ø¨Ù‚Ø© Ø¥Ù„Ù‰ Ø·Ø¨Ù‚Ø© ØªÙ‚\n",
      "----------------------------------------\n",
      "\n",
      "âœ… Darija Q&A fine-tuning complete!\n",
      "ðŸ“ Model saved to: smollm_darija_qa_finetuned\n",
      "ðŸ‡²ðŸ‡¦ Your SmolLM2 now speaks Moroccan Darija with Q&A capabilities!\n",
      "ðŸ“Š Trained on 2378 examples\n",
      "\n",
      "ðŸ“– Usage Instructions:\n",
      "from transformers import pipeline\n",
      "pipe = pipeline('text-generation', model='smollm_darija_qa_finetuned')\n",
      "response = pipe('Ø´Ù†Ùˆ Ù‡ÙˆÙ…Ø§ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ø±ÙŠØ§Ø¶Ø©ØŸ')\n"
     ]
    }
   ]
  }
 ]
}
